{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e14892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc,accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Onehot encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import gc\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0bc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075669a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!mkdir input/\\n%cd input/\\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\\n!unzip NewsAggregatorDataset.zip\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!mkdir input/\n",
    "%cd input/\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "!unzip NewsAggregatorDataset.zip\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b9cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
    "#!sed -e 's/\"/'\\''/g' ./input/newsCorpora.csv > ./input/newsCorpora_re.csv\n",
    "#%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4bfbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# データの読込\n",
    "df = pd.read_csv('./input/newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "# データの抽出\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153b043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Europe reaches crunch point on banking union</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ECB FOCUS-Stronger euro drowns out ECB's messa...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Euro Anxieties Wane as Bunds Top Treasuries, S...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Noyer Says Strong Euro Creates Unwarranted Eco...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>REFILE-Bad loan triggers key feature in ECB ba...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE CATEGORY\n",
       "12       Europe reaches crunch point on banking union        b\n",
       "13  ECB FOCUS-Stronger euro drowns out ECB's messa...        b\n",
       "19  Euro Anxieties Wane as Bunds Top Treasuries, S...        b\n",
       "20  Noyer Says Strong Euro Creates Unwarranted Eco...        b\n",
       "29  REFILE-Bad loan triggers key feature in ECB ba...        b"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f18bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               TITLE CATEGORY\n",
      "0  REFILE-UPDATE 1-European car sales up for sixt...        b\n",
      "1  Amazon Plans to Fight FTC Over Mobile-App Purc...        t\n",
      "2  Kids Still Get Codeine In Emergency Rooms Desp...        m\n",
      "3  What On Earth Happened Between Solange And Jay...        e\n",
      "4  NATO Missile Defense Is Flight Tested Over Hawaii        b\n"
     ]
    }
   ],
   "source": [
    "# データの分割\n",
    "df_train, df_valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "df_valid, df_test = train_test_split(df_valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=df_valid_test['CATEGORY'])\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_valid.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "327beed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n",
    "                        stop_words= None,ngram_range=(1,3),dtype=np.float32)\n",
    "vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n",
    "                        stop_words= None,ngram_range=(3,6),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "117cfb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = vect_word.fit_transform(df_train['TITLE'])\n",
    "vl_vect = vect_word.transform(df_valid['TITLE'])\n",
    "ts_vect = vect_word.transform(df_test['TITLE'])\n",
    "\n",
    "# Character n gram vector\n",
    "tr_vect_char = vect_char.fit_transform(df_train['TITLE'])\n",
    "vl_vect_char = vect_char.transform(df_valid['TITLE'])\n",
    "ts_vect_char = vect_char.transform(df_test['TITLE'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3207d2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 20000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8fd17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.hstack([tr_vect, tr_vect_char])\n",
    "x_val = sparse.hstack([vl_vect, vl_vect_char])\n",
    "x_test = sparse.hstack([ts_vect, ts_vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11e34b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(df_train['CATEGORY'].values)\n",
    "y_vl = le.transform(df_valid['CATEGORY'].values)\n",
    "y_te = le.transform(df_test['CATEGORY'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa013d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X = svd.fit_transform(tr_vect)\n",
    "x_val = svd.transform(vl_vect)\n",
    "x_test = svd.transform(ts_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d702ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b77155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1336,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80806b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1406885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:14:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.fit(X, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a801adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8450598802395209\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(x_val)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "647b68eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9338559146568496\n",
      "0.5067125110771251\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_vl, y_pred, multi_class='ovo'))\n",
    "print(log_loss(y_vl, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4a42c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8555389221556886\n"
     ]
    }
   ],
   "source": [
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(x_test)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a49e707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9422619609725721\n",
      "0.46472405979445847\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_te, y_pred, multi_class='ovo'))\n",
    "print(log_loss(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4146405",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9da21a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f034be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w2v, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.w2v = w2v\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = set(self.w2v.vocab.keys())\n",
    "        self.embedding_dim = self.w2v.vector_size\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in self.tokenizer(text):\n",
    "            if word in self.vocab:\n",
    "                vectors.append(self.w2v[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "    def max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.max(word_embeddings, axis=0)\n",
    "\n",
    "    def concat_average_max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n",
    "\n",
    "    def hierarchical_pooling(self, text, n):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "\n",
    "        text_len = word_embeddings.shape[0]\n",
    "        if n > text_len:\n",
    "            raise ValueError(f\"window size must be less than text length / window_size:{n} text_length:{text_len}\")\n",
    "        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n",
    "\n",
    "        return np.max(window_average_pooling_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57740402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(\n",
    "    './input/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "swem = SWEM(wv, nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb07d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f3f9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3791da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b710bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4501\n",
       "1    4235\n",
       "3    1220\n",
       "2     728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_tr).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39dfc4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:43:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.fit(X, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6d72b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4004\n",
      "0.5017\n",
      "1.9219\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db8545c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4266\n",
      "0.5171\n",
      "1.8261\n"
     ]
    }
   ],
   "source": [
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ec47f",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19c0bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVeダウンロード\n",
    "#!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0cef2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE='./input/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa55be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the glove word vectors (space delimited strings) into a dictionary from word->vector.\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08b4107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 13:46:59.097475: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-16 13:46:59.097564: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f90a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open(EMBEDDING_FILE,'r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e692e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_Glove():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic['a'].shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            if word in self.dic:\n",
    "                vectors.append(self.dic[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "swem = SWEM_Glove(embedding_dict, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef661c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7b8e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.fit(X, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f237cfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4199\n",
      "0.5247\n",
      "1.8890\n",
      "0.4476\n",
      "0.5363\n",
      "1.8704\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))\n",
    "\n",
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb8b4a",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcd68f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea960125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model2 = FastText.load_fasttext_format('cc.en.300.bin')\n",
    "FASTTEXT_MODEL_BIN = \"input/cc.en.300.bin\"\n",
    "#this works\n",
    "ft_model = fasttext.load_model(FASTTEXT_MODEL_BIN)\n",
    "ft_model.get_word_vector(\"additional\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b1e2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b48a6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_FastText():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic['a'].shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            if word in self.dic:\n",
    "                vectors.append(self.dic[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "swem = SWEM_FastText(ft_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6286789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d01fc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(df_train['CATEGORY'].values)\n",
    "y_vl = le.transform(df_valid['CATEGORY'].values)\n",
    "y_te = le.transform(df_test['CATEGORY'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85d534c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:08:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3bbe240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8728\n",
      "0.9476\n",
      "0.4293\n",
      "0.8900\n",
      "0.9548\n",
      "0.3723\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))\n",
    "\n",
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90580b6e",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5974d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 13:22:19.719065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-16 13:22:19.719136: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class BertSequenceVectorizer:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", max_len=128):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n",
    "        self.bert_model = self.bert_model.to(self.device)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def vectorize(self, sentence: str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "\n",
    "        if len_inp >= self.max_len:\n",
    "            inputs = inp[:self.max_len]\n",
    "            masks = [1] * self.max_len\n",
    "        else:\n",
    "            inputs = inp + [0] * (self.max_len - len_inp)\n",
    "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
    "\n",
    "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
    "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
    "\n",
    "        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
    "        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n",
    "\n",
    "        if torch.cuda.is_available():    \n",
    "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
    "        else:\n",
    "            return seq_out[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf28843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d67af1e1d74b6eaa5ac3eeae24ee80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f92cb9ef35437ebfba7c2155bb186a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e81e18109e441699665609e711646ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c06e2fc20141208d7408d1a019be14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5344247385403eb8a315c68a189527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BSV = BertSequenceVectorizer(\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a15b5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_BERT():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic.vectorize('a').shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            vectors.append(self.dic.vectorize(word))\n",
    "            #else:\n",
    "            #   vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "            #                                     self.oov_initialize_range[1],\n",
    "             #                                    self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "swem = SWEM_BERT(BSV, BSV.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(df_train['CATEGORY'].values)\n",
    "y_vl = le.transform(df_valid['CATEGORY'].values)\n",
    "y_te = le.transform(df_test['CATEGORY'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a769fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
