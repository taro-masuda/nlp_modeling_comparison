{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e14892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc,accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Onehot encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import gc\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0bc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075669a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!mkdir input/\\n%cd input/\\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\\n!unzip NewsAggregatorDataset.zip\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!mkdir input/\n",
    "%cd input/\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "!unzip NewsAggregatorDataset.zip\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b9cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
    "#!sed -e 's/\"/'\\''/g' ./input/newsCorpora.csv > ./input/newsCorpora_re.csv\n",
    "#%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4bfbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# データの読込\n",
    "df = pd.read_csv('./input/newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "# データの抽出\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153b043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Europe reaches crunch point on banking union</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ECB FOCUS-Stronger euro drowns out ECB's messa...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Euro Anxieties Wane as Bunds Top Treasuries, S...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Noyer Says Strong Euro Creates Unwarranted Eco...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>REFILE-Bad loan triggers key feature in ECB ba...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE CATEGORY\n",
       "12       Europe reaches crunch point on banking union        b\n",
       "13  ECB FOCUS-Stronger euro drowns out ECB's messa...        b\n",
       "19  Euro Anxieties Wane as Bunds Top Treasuries, S...        b\n",
       "20  Noyer Says Strong Euro Creates Unwarranted Eco...        b\n",
       "29  REFILE-Bad loan triggers key feature in ECB ba...        b"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f18bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               TITLE CATEGORY\n",
      "0  REFILE-UPDATE 1-European car sales up for sixt...        b\n",
      "1  Amazon Plans to Fight FTC Over Mobile-App Purc...        t\n",
      "2  Kids Still Get Codeine In Emergency Rooms Desp...        m\n",
      "3  What On Earth Happened Between Solange And Jay...        e\n",
      "4  NATO Missile Defense Is Flight Tested Over Hawaii        b\n"
     ]
    }
   ],
   "source": [
    "# データの分割\n",
    "df_train, df_valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "df_valid, df_test = train_test_split(df_valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=df_valid_test['CATEGORY'])\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_valid.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327beed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n",
    "                        stop_words= None,ngram_range=(1,3),dtype=np.float32)\n",
    "vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n",
    "                        stop_words=None,ngram_range=(3,6),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117cfb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = vect_word.fit_transform(df_train['TITLE'])\n",
    "vl_vect = vect_word.transform(df_valid['TITLE'])\n",
    "ts_vect = vect_word.transform(df_test['TITLE'])\n",
    "\n",
    "# Character n gram vector\n",
    "tr_vect_char = vect_char.fit_transform(df_train['TITLE'])\n",
    "vl_vect_char = vect_char.transform(df_valid['TITLE'])\n",
    "ts_vect_char = vect_char.transform(df_test['TITLE'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3207d2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 20000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8fd17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.hstack([tr_vect, tr_vect_char])\n",
    "x_val = sparse.hstack([vl_vect, vl_vect_char])\n",
    "x_test = sparse.hstack([ts_vect, ts_vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e34b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(df_train['CATEGORY'].values)\n",
    "y_vl = le.transform(df_valid['CATEGORY'].values)\n",
    "y_te = le.transform(df_test['CATEGORY'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa013d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X = svd.fit_transform(tr_vect)\n",
    "x_val = svd.transform(vl_vect)\n",
    "x_test = svd.transform(ts_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d702ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7b77155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1336,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80806b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1406885f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(X, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a801adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8495508982035929\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(x_val)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647b68eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9354540656488748\n",
      "0.4451635579091488\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_vl, y_pred, multi_class='ovo'))\n",
    "print(log_loss(y_vl, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b4a42c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607784431137725\n"
     ]
    }
   ],
   "source": [
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(x_test)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a49e707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466756742685973\n",
      "0.4065042787507156\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_te, y_pred, multi_class='ovo'))\n",
    "print(log_loss(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4146405",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9da21a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64e6b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f034be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w2v, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.w2v = w2v\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = set(self.w2v.vocab.keys())\n",
    "        self.embedding_dim = self.w2v.vector_size\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in self.tokenizer(text):\n",
    "            if word in self.vocab:\n",
    "                vectors.append(self.w2v[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "    def max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.max(word_embeddings, axis=0)\n",
    "\n",
    "    def concat_average_max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n",
    "\n",
    "    def hierarchical_pooling(self, text, n):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "\n",
    "        text_len = word_embeddings.shape[0]\n",
    "        if n > text_len:\n",
    "            raise ValueError(f\"window size must be less than text length / window_size:{n} text_length:{text_len}\")\n",
    "        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n",
    "\n",
    "        return np.max(window_average_pooling_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "57740402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "swem = SWEM(wv, nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cb07d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d4f3f9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 300)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d3791da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2b710bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4501\n",
       "1    4235\n",
       "3    1220\n",
       "2     728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_tr).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39dfc4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6d72b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4835\n",
      "0.5508\n",
      "1.1765\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "db8545c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5037\n",
      "0.5648\n",
      "1.1567\n"
     ]
    }
   ],
   "source": [
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ec47f",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "19c0bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-12 10:41:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-12-12 10:41:53--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  4.29MB/s    in 3m 31s  \n",
      "\n",
      "2021-12-12 10:45:24 (3.91 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# GloVeダウンロード\n",
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d0cef2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE='./glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa55be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the glove word vectors (space delimited strings) into a dictionary from word->vector.\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08b4107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 14:43:01.260372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-12 14:43:01.262445: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9f90a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open(EMBEDDING_FILE,'r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8e692e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_Glove():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic['a'].shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            if word in self.dic:\n",
    "                vectors.append(self.dic[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "swem = SWEM_Glove(embedding_dict, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ef661c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d7b8e1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f237cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7560\n",
      "0.8353\n",
      "0.7070\n",
      "0.7635\n",
      "0.8543\n",
      "0.6685\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))\n",
    "\n",
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb8b4a",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd68f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "925695ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2pageSessions.csv\t   glove.6B.200d.txt  newsCorpora.csv\r\n",
      "NewsAggregatorDataset.zip  glove.6B.300d.txt  newsCorpora_re.csv\r\n",
      "cc.en.300.bin\t\t   glove.6B.50d.txt   readme.txt\r\n",
      "glove.6B.100d.txt\t   glove.6B.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea960125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model2 = FastText.load_fasttext_format('cc.en.300.bin')\n",
    "FASTTEXT_MODEL_BIN = \"input/cc.en.300.bin\"\n",
    "#this works\n",
    "ft_model = fasttext.load_model(FASTTEXT_MODEL_BIN)\n",
    "ft_model.get_word_vector(\"additional\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b1e2c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 06:31:55.633322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-15 06:31:55.636977: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b48a6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_FastText():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic['a'].shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            if word in self.dic:\n",
    "                vectors.append(self.dic[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "swem = SWEM_FastText(ft_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6286789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d01fc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(df_train['CATEGORY'].values)\n",
    "y_vl = le.transform(df_valid['CATEGORY'].values)\n",
    "y_te = le.transform(df_test['CATEGORY'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85d534c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3bbe240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8743\n",
      "0.9503\n",
      "0.3749\n",
      "0.8825\n",
      "0.9528\n",
      "0.3366\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))\n",
    "\n",
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90580b6e",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5974d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 13:22:19.719065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-16 13:22:19.719136: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class BertSequenceVectorizer:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", max_len=128):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n",
    "        self.bert_model = self.bert_model.to(self.device)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def vectorize(self, sentence: str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "\n",
    "        if len_inp >= self.max_len:\n",
    "            inputs = inp[:self.max_len]\n",
    "            masks = [1] * self.max_len\n",
    "        else:\n",
    "            inputs = inp + [0] * (self.max_len - len_inp)\n",
    "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
    "\n",
    "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
    "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
    "\n",
    "        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
    "        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n",
    "\n",
    "        if torch.cuda.is_available():    \n",
    "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
    "        else:\n",
    "            return seq_out[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f77a1d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d67af1e1d74b6eaa5ac3eeae24ee80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f92cb9ef35437ebfba7c2155bb186a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e81e18109e441699665609e711646ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c06e2fc20141208d7408d1a019be14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5344247385403eb8a315c68a189527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BSV = BertSequenceVectorizer(\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a15b5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_BERT():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic.vectorize('a').shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            vectors.append(self.dic.vectorize(word))\n",
    "            #else:\n",
    "            #   vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "            #                                     self.oov_initialize_range[1],\n",
    "             #                                    self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "swem = SWEM_BERT(BSV, BSV.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "076d26c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11/1889258266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtr_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mswem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvl_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mswem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mts_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mswem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11/1889258266.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtr_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mswem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvl_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mswem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mts_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mswem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11/3277314990.py\u001b[0m in \u001b[0;36maverage_pooling\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maverage_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11/3277314990.py\u001b[0m in \u001b[0;36mget_word_embeddings\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#   vectors.append(np.random.uniform(self.oov_initialize_range[0],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11/2140766852.py\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmasks_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mbert_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mseq_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pooler_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         )\n\u001b[0;32m--> 999\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 )\n\u001b[1;32m    584\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     ):\n\u001b[0;32m--> 402\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Word vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059889d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(df_train['CATEGORY'].values)\n",
    "y_vl = le.transform(df_valid['CATEGORY'].values)\n",
    "y_te = le.transform(df_test['CATEGORY'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
