{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e44ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc,accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Onehot encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import gc\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4668548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ae53892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘input/’: File exists\n",
      "/lib/modules/input\n",
      "--2021-12-12 09:05:35--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29224203 (28M) [application/x-httpd-php]\n",
      "Saving to: ‘NewsAggregatorDataset.zip.1’\n",
      "\n",
      " NewsAggregatorData  19%[==>                 ]   5.32M  2.90MB/s               ^C\n",
      "Archive:  NewsAggregatorDataset.zip\n",
      "replace 2pageSessions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir input/\n",
    "%cd input/\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "!unzip NewsAggregatorDataset.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "064c595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
    "!sed -e 's/\"/'\\''/g' ./input/newsCorpora.csv > ./input/newsCorpora_re.csv\n",
    "#%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e498c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# データの読込\n",
    "df = pd.read_csv('./input/newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "# データの抽出\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432d5434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Europe reaches crunch point on banking union</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ECB FOCUS-Stronger euro drowns out ECB's messa...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Euro Anxieties Wane as Bunds Top Treasuries, S...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Noyer Says Strong Euro Creates Unwarranted Eco...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>REFILE-Bad loan triggers key feature in ECB ba...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE CATEGORY\n",
       "12       Europe reaches crunch point on banking union        b\n",
       "13  ECB FOCUS-Stronger euro drowns out ECB's messa...        b\n",
       "19  Euro Anxieties Wane as Bunds Top Treasuries, S...        b\n",
       "20  Noyer Says Strong Euro Creates Unwarranted Eco...        b\n",
       "29  REFILE-Bad loan triggers key feature in ECB ba...        b"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c83f0e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               TITLE CATEGORY\n",
      "0  REFILE-UPDATE 1-European car sales up for sixt...        b\n",
      "1  Amazon Plans to Fight FTC Over Mobile-App Purc...        t\n",
      "2  Kids Still Get Codeine In Emergency Rooms Desp...        m\n",
      "3  What On Earth Happened Between Solange And Jay...        e\n",
      "4  NATO Missile Defense Is Flight Tested Over Hawaii        b\n"
     ]
    }
   ],
   "source": [
    "# データの分割\n",
    "df_train, df_valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "df_valid, df_test = train_test_split(df_valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=df_valid_test['CATEGORY'])\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_valid.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb71c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',ngram_range=(1,3),dtype=np.float32)\n",
    "vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n",
    "                        stop_words= 'english',ngram_range=(3,6),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b71221e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = vect_word.fit_transform(df_train['TITLE'])\n",
    "vl_vect = vect_word.transform(df_valid['TITLE'])\n",
    "ts_vect = vect_word.transform(df_test['TITLE'])\n",
    "\n",
    "# Character n gram vector\n",
    "tr_vect_char = vect_char.fit_transform(df_train['TITLE'])\n",
    "vl_vect_char = vect_char.transform(df_valid['TITLE'])\n",
    "ts_vect_char = vect_char.transform(df_test['TITLE'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58fddc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 20000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e528426",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.hstack([tr_vect, tr_vect_char])\n",
    "x_val = sparse.hstack([vl_vect, vl_vect_char])\n",
    "x_test = sparse.hstack([ts_vect, ts_vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "445cb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(df_train['CATEGORY'].values)\n",
    "y_vl = le.transform(df_valid['CATEGORY'].values)\n",
    "y_te = le.transform(df_test['CATEGORY'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddb4102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X = svd.fit_transform(tr_vect)\n",
    "x_val = svd.transform(vl_vect)\n",
    "x_test = svd.transform(ts_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6ce2c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "341405b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1336, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "578fc5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed7b5439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(X, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2357f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8787425149700598\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(x_val)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4d97658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9514706810553059\n",
      "0.3888652444277943\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_vl, y_pred, multi_class='ovo'))\n",
    "print(log_loss(y_vl, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eaa04dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8839820359281437\n"
     ]
    }
   ],
   "source": [
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(x_test)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bd82583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9547788083432888\n",
      "0.36538368891460654\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_te, y_pred, multi_class='ovo'))\n",
    "print(log_loss(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4146405",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9da21a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64e6b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f034be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w2v, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.w2v = w2v\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = set(self.w2v.vocab.keys())\n",
    "        self.embedding_dim = self.w2v.vector_size\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in self.tokenizer(text):\n",
    "            if word in self.vocab:\n",
    "                vectors.append(self.w2v[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "    def max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.max(word_embeddings, axis=0)\n",
    "\n",
    "    def concat_average_max_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n",
    "\n",
    "    def hierarchical_pooling(self, text, n):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "\n",
    "        text_len = word_embeddings.shape[0]\n",
    "        if n > text_len:\n",
    "            raise ValueError(f\"window size must be less than text length / window_size:{n} text_length:{text_len}\")\n",
    "        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n",
    "\n",
    "        return np.max(window_average_pooling_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "57740402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "swem = SWEM(wv, nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cb07d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d4f3f9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 300)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d3791da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2b710bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4501\n",
       "1    4235\n",
       "3    1220\n",
       "2     728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_tr).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39dfc4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6d72b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4835\n",
      "0.5508\n",
      "1.1765\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "db8545c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5037\n",
      "0.5648\n",
      "1.1567\n"
     ]
    }
   ],
   "source": [
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ec47f",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "19c0bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-12 10:41:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-12-12 10:41:53--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  4.29MB/s    in 3m 31s  \n",
      "\n",
      "2021-12-12 10:45:24 (3.91 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# GloVeダウンロード\n",
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d0cef2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE='./glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa55be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the glove word vectors (space delimited strings) into a dictionary from word->vector.\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08b4107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 14:43:01.260372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-12 14:43:01.262445: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9f90a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open(EMBEDDING_FILE,'r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8e692e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_Glove():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic['a'].shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            if word in self.dic:\n",
    "                vectors.append(self.dic[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "swem = SWEM_Glove(embedding_dict, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ef661c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ngram vector\n",
    "tr_vect = np.array([swem.average_pooling(text) for text in df_train['TITLE'].tolist()])\n",
    "vl_vect = np.array([swem.average_pooling(text) for text in df_valid['TITLE'].tolist()])\n",
    "ts_vect = np.array([swem.average_pooling(text) for text in df_test['TITLE'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d7b8e1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(tr_vect, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f237cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7560\n",
      "0.8353\n",
      "0.7070\n",
      "0.7635\n",
      "0.8543\n",
      "0.6685\n"
     ]
    }
   ],
   "source": [
    "# 検証データを予測する\n",
    "y_pred = model.predict_proba(vl_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_vl == y_pred_max) / len(y_vl)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_vl, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_vl, y_pred)))\n",
    "\n",
    "# 評価データを予測する\n",
    "y_pred = model.predict_proba(ts_vect)\n",
    "y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
    "\n",
    "accuracy = sum(y_te == y_pred_max) / len(y_te)\n",
    "print('{:.4f}'.format(accuracy))\n",
    "print('{:.4f}'.format(roc_auc_score(y_te, y_pred, multi_class='ovo')))\n",
    "print('{:.4f}'.format(log_loss(y_te, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb8b4a",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd68f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113/1052393370.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea960125",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'FastText' has no attribute 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113/2477324150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFASTTEXT_MODEL_BIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fasttext_model.bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#this works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mft_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFASTTEXT_MODEL_BIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"additional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'FastText' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "#model2 = FastText.load_fasttext_format('cc.en.300.bin')\n",
    "FASTTEXT_MODEL_BIN = \"fasttext_model.bin\"\n",
    "#this works\n",
    "ft_model = FastText.load_model(FASTTEXT_MODEL_BIN)\n",
    "ft_model.get_word_vector(\"additional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv['computer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM_FastText():\n",
    "    \"\"\"\n",
    "    Simple Word-Embeddingbased Models (SWEM)\n",
    "    https://arxiv.org/abs/1805.09843v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dic = dic\n",
    "        self.embedding_dim = self.dic['a'].shape[0]\n",
    "        self.oov_initialize_range = oov_initialize_range\n",
    "\n",
    "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
    "            raise ValueError(\"Specify valid initialize range: \"\n",
    "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
    "\n",
    "    def get_word_embeddings(self, text):\n",
    "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "\n",
    "        vectors = []\n",
    "        for word in text.split():\n",
    "            if word in self.dic:\n",
    "                vectors.append(self.dic[word])\n",
    "            else:\n",
    "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
    "                                                 self.oov_initialize_range[1],\n",
    "                                                 self.embedding_dim))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_word_embeddings(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "swem = SWEM_Glove(embedding_dict, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
